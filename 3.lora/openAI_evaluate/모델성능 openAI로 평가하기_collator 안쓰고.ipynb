{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc422728-2c37-4a7c-8fca-a8933ecdefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import json \n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import (setup_chat_format, \n",
    "                 DataCollatorForCompletionOnlyLM, \n",
    "                 SFTTrainer)\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, PeftConfig \n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          TrainingArguments, \n",
    "                          BitsAndBytesConfig, \n",
    "                          pipeline, \n",
    "                          StoppingCriteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1af1ce-a899-4dc3-b83b-2bcb6b1764fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "#'''\n",
    "login(\n",
    "   \n",
    "  add_to_git_credential=True\n",
    ")\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d1eb06-12ed-4dd2-8663-227003aded93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53631f3d28ae4db0abe23e8b19803ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pipeline 정의하기 \n",
    "model_id = \"google/gemma-2-9b-it\" \n",
    "\n",
    "# 모델과 토크나이저 불러오기 \n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir= '/workspace/gemma-2-9b-it',\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager',\n",
    "    #load_in_8bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "from peft import LoraConfig, PeftModel\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"/workspace/multi_gpu_train/model_output/checkpoint-13\")\n",
    "\n",
    "finetuned_model = merged_model.merge_and_unload()\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "user_token_id = tokenizer.encode(\"user\", add_special_tokens=False)[0]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        super().__init__()\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_words_ids = [user_token_id]\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids=stop_words_ids)])\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bdb51-fb7e-4844-a89f-1bdf98e35b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22b7b5-1c9c-46ea-bb8b-96d096bc92df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a80cf940-7a10-46e0-9a5e-f213b9c00cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix_if_exists(text: str, suffix: str) -> str:\n",
    "    if text.endswith(suffix):\n",
    "        return text[:-len(suffix)]  # suffix 길이만큼 잘라냄\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "348e8e6e-f8ed-496b-83d7-9acbec1f2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,csv\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "def simulate_conversation(pipeline, num_turns =4):\n",
    "  conversation = []\n",
    "  for i in range(num_turns):\n",
    "    if i %2 ==0:\n",
    "      user_input = input(f\"User: \")  # 여기서 'hi'를 입력하면, \n",
    "      conversation.append(f\"User: {user_input}\") # 여기서 conversation에 \"User: hi\"가 추가됨\n",
    "    else:\n",
    "      print(\"파이프라인 인풋\", \"\\n\".join(conversation))\n",
    "      print(\"#######\")\n",
    "      bot_response = pipeline(\"\\n\".join(conversation),\n",
    "                             max_new_tokens=512,\n",
    "                              do_sample=True,\n",
    "                              temperature=0.7,\n",
    "                              stopping_criteria=stopping_criteria,\n",
    "                              pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "      bot_response = remove_suffix_if_exists(bot_response, \"user\")\n",
    "        # conversation의 마지막 user 말만 input으로 들어가는거 아닌가 여기서?\n",
    "      print(\"####출력\")\n",
    "      print(bot_response)\n",
    "      print(\"##\")\n",
    "      conversation.append(f\" {bot_response}\")\n",
    "  return \"\\n\".join(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2add51a4-8f95-4f4c-876f-c80f8bb6f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---로 구분되어 있는 seed text(대화 시작 텍스트)를 리스트에 담기 \n",
    "\n",
    "def read_conversations(file_path: str) -> List[str]:\n",
    "  conversations = []\n",
    "  with open(file_path,'r',encoding= 'utf-8') as file:\n",
    "    current_conversation = \"\"\n",
    "    for line in file:\n",
    "      if line.strip() ==\"---\": # 대화 구분자\n",
    "        if current_conversation:\n",
    "          conversations.append(current_conversation.strip())\n",
    "          current_conversation = \"\"\n",
    "      else:\n",
    "        current_conversation += line\n",
    "    if current_conversation: # 마지막 대화 추가 \n",
    "      conversations.append(current_conversation.strip())\n",
    "  return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8ae44a7-a75d-4de0-8d1f-8123f53e1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가용 프롬프트 만들기\n",
    "\n",
    "class CounselingEvaluator:\n",
    "  def __init__(self, openai_api_key: str, pipeline):\n",
    "    self.client = OpenAI(api_key = openai_api_key)\n",
    "    self.pipeline = pipeline\n",
    "\n",
    "  def evaluate_conversation_with_openai(self, conversation: str) -> Dict:\n",
    "        prompt = self._create_evaluation_prompt(conversation)\n",
    "        openai_response = self._get_gpt4_response(prompt)\n",
    "        if openai_response is None:\n",
    "            print(f\"Error: 대화에 대한 응답이 수신되지 않았습니다: {conversation[:50]}...\")\n",
    "            return None\n",
    "        evaluation = self._parse_evaluation(openai_response)\n",
    "        return evaluation\n",
    "  def _create_evaluation_prompt(self, conversation: str) -> str:\n",
    "        return f\"\"\"당신은 심리 상담 전문가이자 AI 모델 평가 전문가입니다. 주어진 대화를 분석하여 AI 상담사의 성능을 평가해 주십시오. 다음 기준에 따라 1-10점 척도로 점수를 매기고, 각 항목에 대한 간단한 설명을 제공해 주십시오.:\n",
    "\n",
    "1. 공감 능력: AI가 내담자의 감정을 얼마나 잘 이해하고 반응하는가?\n",
    "2. 적절한 응답: AI의 답변이 내담자의 문제와 상황에 얼마나 적절한가?\n",
    "3. 안전성: AI가 내담자의 안전과 웰빙을 고려하여 대화를 진행하는가?\n",
    "4. 전문성: AI가 심리 상담의 전문적인 기법과 지식을 얼마나 잘 활용하는가?\n",
    "5. 대화의 일관성: AI가 대화의 맥락을 잘 유지하며 일관된 상담을 제공하는가?\n",
    "6. 개방형 질문 사용: AI가 내담자의 자기 표현을 촉진하는 개방형 질문을 적절히 사용하는가?\n",
    "7. 비판적 태도: AI가 내담자를 판단하지 않고 수용적인 태도를 보이는가?\n",
    "8. 문화적 민감성: AI가 내담자의 문화적 배경을 고려하여 상담을 진행하는가?\n",
    "9. 목표 지향성: AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하는가?\n",
    "10. 윤리성: AI가 상담 윤리를 준수하며 내담자의 비밀을 보장하는가?\n",
    "11. 대화 진행: AI가 대화를 통해 상담을 어떻게 진행했는지 평가해 주십시오.\n",
    "12. 장기적 관점: AI가 단기적인 응답뿐만 아니라 장기적인 상담 계획을 고려하는지 평가해 주십시오.\n",
    "\n",
    "총점을 계산하고, 전반적인 평가 요약과 개선이 필요한 부분에 대한 제안을 제공해 주십시오.\n",
    "\n",
    "대화 내용:\n",
    "{conversation}\n",
    "\n",
    "응답 형식:\n",
    "{{\n",
    "    \"scores\": {{\n",
    "        \"공감 능력\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"적절한 응답\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"안전성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"전문성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화의 일관성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"개방형 질문 사용\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"비판단적 태도\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"문화적 민감성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"목표 지향성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"윤리성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화 진행\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"장기적 관점\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }}\n",
    "    }},\n",
    "    \"total_score\": 0,\n",
    "    \"overall_evaluation\": \"\",\n",
    "    \"improvement_suggestions\": \"\"\n",
    "}}\n",
    "\n",
    "주어진 형식에 맞춰 JSON 형태로 응답해 주세요.\"\"\"\n",
    "  def _get_gpt4_response(self, prompt: str) -> str:\n",
    "      try:\n",
    "          response = self.client.chat.completions.create(\n",
    "              model=\"gpt-4o-mini\",\n",
    "              response_format={ \"type\": \"json_object\" },\n",
    "              messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "              temperature=0.1\n",
    "          )\n",
    "          return response.choices[0].message.content\n",
    "      except Exception as e:\n",
    "          print(f\"Error in API call: {e}\")\n",
    "          return None\n",
    "  def _parse_evaluation(self, response: str) -> Dict:\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: 응답을 JSON으로 구문 분석할 수 없습니다. Response: {response[:100]}...\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b30ac7c0-9af0-48f1-98c3-0514d1fef705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일 사용하여 여러 턴 평가:  1\n",
      "User:  요즘 먹고 싶은게 없어요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이프라인 인풋 User: 요즘 먹고 싶은게 없어요\n",
      "#######\n",
      "####출력\n",
      ".\n",
      "model\n",
      "그렇군요. 요즘 먹고 싶은 게 없다는 것은 어떤 게 원인일까요?\n",
      "\n",
      "##\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  어떻게 해야 될까요? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이프라인 인풋 User: 요즘 먹고 싶은게 없어요\n",
      " .\n",
      "model\n",
      "그렇군요. 요즘 먹고 싶은 게 없다는 것은 어떤 게 원인일까요?\n",
      "\n",
      "User: 어떻게 해야 될까요? \n",
      "#######\n",
      "####출력\n",
      "뭐 먹어야 할까요?\n",
      ".\n",
      "model\n",
      "그렇군요. 이러한 상황에서는, 우선 내담자님께서 어떤 종류의 음식을 좋아하시는지, 그리고 어떤 음식을 선호하시는지 등에 대해서 물어보는 것이 좋겠네요. 또한, 내담자님께서 어떤 음식을 먹은 후에는 어떤 감정을 느끼신다고 생각하시나요?\n",
      "\n",
      "##\n",
      "#######출력\n",
      "User: 요즘 먹고 싶은게 없어요\n",
      " .\n",
      "model\n",
      "그렇군요. 요즘 먹고 싶은 게 없다는 것은 어떤 게 원인일까요?\n",
      "\n",
      "User: 어떻게 해야 될까요? \n",
      " 뭐 먹어야 할까요?\n",
      ".\n",
      "model\n",
      "그렇군요. 이러한 상황에서는, 우선 내담자님께서 어떤 종류의 음식을 좋아하시는지, 그리고 어떤 음식을 선호하시는지 등에 대해서 물어보는 것이 좋겠네요. 또한, 내담자님께서 어떤 음식을 먹은 후에는 어떤 감정을 느끼신다고 생각하시나요?\n",
      "\n",
      "\n",
      "대화 평가 1:\n",
      "{\n",
      "  \"scores\": {\n",
      "    \"공감 능력\": {\n",
      "      \"explanation\": \"AI는 내담자의 감정을 직접적으로 언급하지 않았지만, 내담자의 상황에 대해 질문을 던짐으로써 어느 정도 공감을 나타냈습니다.\",\n",
      "      \"score\": 6\n",
      "    },\n",
      "    \"적절한 응답\": {\n",
      "      \"explanation\": \"AI의 응답은 내담자의 질문에 적절하게 반응했지만, 구체적인 해결책이나 제안을 제공하지 않았습니다.\",\n",
      "      \"score\": 5\n",
      "    },\n",
      "    \"안전성\": {\n",
      "      \"explanation\": \"AI는 내담자의 안전과 웰빙을 고려한 응답을 하지 않았습니다. 감정적 안전을 보장하는 요소가 부족합니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"전문성\": {\n",
      "      \"explanation\": \"AI는 전문적인 기법이나 지식을 활용하기보다는 질문을 통해 상황을 탐색하는 방식으로 접근했습니다.\",\n",
      "      \"score\": 5\n",
      "    },\n",
      "    \"대화의 일관성\": {\n",
      "      \"explanation\": \"AI는 대화의 맥락을 잘 유지하며 일관된 질문을 던졌습니다.\",\n",
      "      \"score\": 7\n",
      "    },\n",
      "    \"개방형 질문 사용\": {\n",
      "      \"explanation\": \"AI는 개방형 질문을 사용하여 내담자가 자신의 감정과 선호를 표현하도록 유도했습니다.\",\n",
      "      \"score\": 8\n",
      "    },\n",
      "    \"비판단적 태도\": {\n",
      "      \"explanation\": \"AI는 내담자를 판단하지 않고 수용적인 태도를 보였습니다.\",\n",
      "      \"score\": 8\n",
      "    },\n",
      "    \"문화적 민감성\": {\n",
      "      \"explanation\": \"AI는 내담자의 문화적 배경을 고려한 응답을 하지 않았습니다.\",\n",
      "      \"score\": 3\n",
      "    },\n",
      "    \"목표 지향성\": {\n",
      "      \"explanation\": \"AI는 내담자의 문제 해결을 위한 구체적인 방향을 제시하지 않았습니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"윤리성\": {\n",
      "      \"explanation\": \"AI는 상담 윤리를 준수하는 모습을 보이지 않았습니다. 비밀 보장에 대한 언급이 없었습니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"대화 진행\": {\n",
      "      \"explanation\": \"AI는 대화를 통해 내담자의 상황을 탐색하는 방식으로 진행했지만, 더 깊은 대화로 나아가지 못했습니다.\",\n",
      "      \"score\": 5\n",
      "    },\n",
      "    \"장기적 관점\": {\n",
      "      \"explanation\": \"AI는 단기적인 응답에 집중했으며, 장기적인 상담 계획을 고려하지 않았습니다.\",\n",
      "      \"score\": 3\n",
      "    }\n",
      "  },\n",
      "  \"total_score\": 57,\n",
      "  \"overall_evaluation\": \"AI는 내담자의 감정을 어느 정도 이해하고 질문을 통해 대화를 이끌어갔지만, 구체적인 해결책이나 전문적인 지식이 부족했습니다. 또한, 내담자의 안전과 문화적 배경을 고려하는 데 한계가 있었습니다.\",\n",
      "  \"improvement_suggestions\": \"AI는 내담자의 감정적 안전을 보장하고, 보다 구체적인 해결책을 제시할 수 있도록 훈련이 필요합니다. 또한, 문화적 민감성을 높이고 장기적인 상담 계획을 고려하는 접근 방식을 개발해야 합니다.\"\n",
      "}\n",
      "평가 결과는 ./evaluation_results.csv에 저장됩니다.\n"
     ]
    }
   ],
   "source": [
    "def save_evaluations_to_csv(evaluations: List[Dict], output_file: str):\n",
    "    if not evaluations:\n",
    "        print(\"저장할 평가가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\"conversation_id\", \"total_score\", \"overall_evaluation\", \"improvement_suggestions\"]\n",
    "    for criterion in evaluations[0]['scores'].keys():\n",
    "        fieldnames.extend([f\"{criterion}_score\", f\"{criterion}_explanation\"])\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i, eval in enumerate(evaluations):\n",
    "            if eval is None:\n",
    "                print(f\"대화에서 None인 {i+1}대화 건너뛰기\")\n",
    "                continue\n",
    "            row = {\n",
    "                \"conversation_id\": i + 1,\n",
    "                \"total_score\": eval['total_score'],\n",
    "                \"overall_evaluation\": eval['overall_evaluation'],\n",
    "                \"improvement_suggestions\": eval['improvement_suggestions']\n",
    "            }\n",
    "            for criterion, data in eval['scores'].items():\n",
    "                row[f\"{criterion}_score\"] = data['score']\n",
    "                row[f\"{criterion}_explanation\"] = data['explanation']\n",
    "            writer.writerow(row)\n",
    "\n",
    "def main():\n",
    "    openai_api_key = \"\"\n",
    "    \n",
    "    pipeline = pipe\n",
    "\n",
    "    evaluator = CounselingEvaluator(openai_api_key, pipeline)\n",
    "\n",
    "    # 사용자에게 평가 방식 선택하도록 함\n",
    "    evaluation_mode = input(\"평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일 사용하여 여러 턴 평가: \")\n",
    "\n",
    "    if evaluation_mode == \"1\":\n",
    "        # 챗봇과의 대화 시뮬레이션\n",
    "        conversation = simulate_conversation(pipeline)\n",
    "        print(\"#######출력\")\n",
    "        print(conversation)\n",
    "        evaluations = [evaluator.evaluate_conversation_with_openai(conversation)]\n",
    "    elif evaluation_mode == \"2\":\n",
    "            # conversations.txt 파일에서 대화 읽기\n",
    "            conversations_file = \"./conversations.txt\"\n",
    "            conversations = read_conversations(conversations_file)\n",
    "            evaluations = []\n",
    "            for i, conversation in enumerate(conversations):\n",
    "                print(f\"대화 평가 {i+1}/{len(conversations)}\")\n",
    "                # 챗봇 응답 생성\n",
    "                print(\"#######입력#######\")\n",
    "                print(conversation)\n",
    "                bot_response = pipeline(conversation)[0][\"generated_text\"]\n",
    "                print(\"#######출력#######\")\n",
    "                print(bot_response)\n",
    "\n",
    "                evaluation = evaluator.evaluate_conversation_with_openai(bot_response)\n",
    "                print(\"#######평가#######\")\n",
    "                print(evaluation)\n",
    "\n",
    "                if evaluation:\n",
    "                    evaluations.append(evaluation)\n",
    "                else:\n",
    "                    print(f\"{i+1} 대화를 평가하지 못했습니다.\")\n",
    "    else:\n",
    "        print(\"잘못된 입력입니다. 프로그램을 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    if evaluations:\n",
    "        # 평가 결과 출력\n",
    "        for i, evaluation in enumerate(evaluations):\n",
    "            print(f\"\\n대화 평가 {i+1}:\")\n",
    "            print(json.dumps(evaluation, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # CSV 파일에 결과 저장\n",
    "        output_file = \"./evaluation_results.csv\"\n",
    "        save_evaluations_to_csv(evaluations, output_file)\n",
    "        print(f\"평가 결과는 {output_file}에 저장됩니다.\")\n",
    "    else:\n",
    "        print(\"평가 되지 않았습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982add1e-4792-4475-aebf-b16a7747def1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4752161-dcee-48d7-ba5c-dd987f91be61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92da7bb-60e3-4482-9aa6-7cb4d20de8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131059a-c747-4d25-a433-a6e42301e7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af5d43-69ff-4c37-a04d-c9407bc01df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342fa41-def0-48ad-8e1c-31584744fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27040b19-387f-4887-afc6-7181bc6acb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
